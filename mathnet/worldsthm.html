'This article is about the theorem as used in time series analysis. For an abstract mathematical statement, see Wold decomposition. In statistics, Wold\'s decomposition or the Wold representation theorem to be confused with the Wold theorem that is the discrete-time analog of the Wiener\u2013Khinchin named after Herman Wold, says that every covariance-stationary time series Y t can be written as the sum of two time series, one deterministic and one stochastic. Formally Y t = \u2211 j = 0 \u221e b j \u03b5 t \u2212 j + \u03b7 t , where: Y t is the time series being considered, \u03b5 t \\varepsilon is an uncorrelated sequence which is the innovation process to the process Y t \u2013 that is, a white noise process that is input to the linear filter b j . b is the possibly infinite vector of moving average weights or \u03b7 t \\eta is a deterministic time series, such as one represented by a sine wave. Note that the moving average coefficients have these properties: Stable, that is square summable \u2211 j = 1 \u221e | b j | 2 \\sum < \u221e \\infty Causal there are no terms with j < Minimum delay[clarification needed] Constant b j independent of It is conventional to define b 0 = 1 This theorem can be considered as an existence theorem: any stationary process has this seemingly special representation. Not only is the existence of such a simple linear and exact representation remarkable, but even more so is the special nature of the moving average model. Imagine creating a process that is a moving average but not satisfying these properties 1\u20134. For example, the coefficients b j could define an acausal and non-minimum delay[clarification needed] model. Nevertheless the theorem assures the existence of a causal minimum delay moving average[clarification needed] that exactly represents this process. How this all works for the case of causality and the minimum delay property is discussed in Scargle where an extension of the Wold Decomposition is discussed. The usefulness of the Wold Theorem is that it allows the dynamic evolution of a variable Y t to be approximated by a linear model. If the innovations \u03b5 t \\varepsilon are independent, then the linear model is the only possible representation relating the observed value of Y t to its past evolution. However, when \u03b5 t \\varepsilon is merely an uncorrelated but not independent sequence, then the linear model exists but it is not the only representation of the dynamic dependence of the series. In this latter case, it is possible that the linear model may not be very useful, and there would be a nonlinear model relating the observed value of Y t to its past evolution. However, in practical time series analysis, it is often the case that only linear predictors are considered, partly on the grounds of simplicity, in which case the Wold decomposition is directly relevant. The Wold representation depends on an infinite number of parameters, although in practice they usually decay rapidly. The autoregressive model is an alternative that may have only a few coefficients if the corresponding moving average has many. These two models can be combined into an autoregressive-moving average model, or an autoregressive-integrated-moving average model if non-stationarity is involved. See Scargle and references there; in addition this paper gives an extension of the Wold Theorem that allows more generality for the moving average necessarily stable, causal, or minimum accompanied by a sharper characterization of the innovation and independently distributed, not just This extension allows the possibility of models that are more faithful to physical or astrophysical processes, and in particular can sense \u2033the arrow of time.\u2033 References[edit] Anderson, T. W. The Statistical Analysis of Time Series. Wiley. Nerlove, M.; Grether, David M.; Carvalho, Jos\xe9 L. Analysis of Economic Time Series San Diego: Academic Press. pp. 30\u201336. ISBN 0-12-515751-7. Scargle, J. D. Studies in astronomical time series analysis. I \u2013 Modeling random processes in the time domain. Astrophysical Journal Supplement Series. 45. pp. 1\u201371. Wold, H. A Study in the Analysis of Stationary Time Series, Second revised edition, with an Appendix on "Recent Developments in Time Series Analysis" by Peter Whittle. Almqvist and Wiksell Book Co., Uppsala. v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean arithmetic geometric harmonic Median Mode Dispersion Variance Standard deviation Coefficient of variation Percentile Range Interquartile range Shape Moments Skewness Kurtosis L-moments Count data Index of dispersion Summary tables Grouped data Frequency distribution Contingency table Dependence Pearson product-moment correlation Rank correlation Spearman\'s rho Kendall\'s tau Partial correlation Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q\u2013Q plot Run chart Scatter plot Stem-and-leaf display Radar chart Data collection Study design Population Statistic Effect size Statistical power Sample size determination Missing data Survey methodology Sampling stratified cluster Standard error Opinion poll Questionnaire Controlled experiments Design control optimal Controlled trial Randomized Random assignment Replication Blocking Interaction Factorial experiment Uncontrolled studies Observational study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Lp space Parameter location scale shape Parametric family Likelihood Location-scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao\u2013Blackwellization Lehmann\u2013Scheff\xe9 theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Wald Score Specific tests Z Student\'s t-test F Goodness of fit Chi-squared Kolmogorov\u2013Smirnov Anderson\u2013Darling Normality Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank Hodges\u2013Lehmann estimator Rank sum Nonparametric anova 1-way 2-way Ordered alternative Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product\u2013moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression model validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Heteroscedasticity Homoscedasticity Generalized linear model Exponential families Logistic / Binomial / Poisson regressions Partition of variance Analysis of variance Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / Multivariate / Time-series / Survival analysis Categorical Cohen\'s kappa Contingency table Graphical model Log-linear model McNemar\'s test Multivariate Regression Anova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey\u2013Fuller Johansen Q-statistic Durbin\u2013Watson Breusch\u2013Godfrey Time domain Autocorrelation partial Cross-correlation ARMA model ARIMA model Autoregressive conditional heteroskedasticity Vector autoregression Frequency domain Spectral density estimation Fourier analysis Wavelet Survival Survival function Kaplan\u2013Meier estimator Proportional hazards models Accelerated failure time model First hitting time Hazard function Nelson\u2013Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Portal Commons WikiProject'